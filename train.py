import torch
import numpy as np
from torch.utils.data import DataLoader
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
from model.SingleStepTransformer import SingleStepTransformer
from model.RecursiveTransformer import RecursiveTransformer, MultiStepTransformer, HybridTransformer
from datasets.data_processor import TimeSeriesDataProcessor, create_data_loaders

# é…ç½®matplotlibä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']  # è®¾ç½®å­—ä½“ä¸ºé»‘ä½“
plt.rcParams['axes.unicode_minus'] = False    # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# ===== å®éªŒé…ç½® =====
PRED_DAYS = 90
BATCH_SIZE = 32  # é™ä½æ‰¹æ¬¡å¤§å°ä»¥é€‚åº”æ›´å¤æ‚çš„æ¨¡å‹
LEARNING_RATE = 3e-4  # è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡
EPOCHS = 200  # å¢åŠ è®­ç»ƒè½®æ•°ä»¥å……åˆ†åˆ©ç”¨æ®‹å·®è¿æ¥
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_EXPERIMENTS = 5  # è¿›è¡Œ5æ¬¡å®éªŒ
TRAINING_STRATEGY = "rolling"  # å¯é€‰: "single", "rolling", "teacher_forcing", "recursive"
MODEL_TYPE = "multi_step"         # å¯é€‰: "single_step", "recursive", "multi_step", "hybrid"
MODEL_PATH = f"{PRED_DAYS}_{MODEL_TYPE}_{TRAINING_STRATEGY}_residual_best_model.pt"

# ================== è®­ç»ƒå‡½æ•° ==================
def train_single_step(model, train_loader, val_loader):
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    loss_fn = torch.nn.SmoothL1Loss()
    best_loss = float('inf')
    for epoch in range(EPOCHS):
        model.train()
        train_losses = []
        for xb, yb in train_loader:
            xb = xb.to(DEVICE)
            yb = yb[:, 0].to(DEVICE)
            optimizer.zero_grad()
            output = model(xb)
            # å¤„ç†ä¸åŒæ¨¡å‹çš„è¾“å‡ºæ ¼å¼
            if isinstance(output, tuple):
                preds = output[0]  # å–é¢„æµ‹å€¼
            else:
                preds = output
            loss = loss_fn(preds, yb)
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())
        model.eval()
        val_losses = []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(DEVICE)
                yb = yb[:, 0].to(DEVICE)
                output = model(xb)
                # å¤„ç†ä¸åŒæ¨¡å‹çš„è¾“å‡ºæ ¼å¼
                if isinstance(output, tuple):
                    preds = output[0]  # å–é¢„æµ‹å€¼
                else:
                    preds = output
                loss = loss_fn(preds, yb)
                val_losses.append(loss.item())
        if np.mean(val_losses) < best_loss:
            best_loss = np.mean(val_losses)
            torch.save(model.state_dict(), MODEL_PATH)
        if epoch % 10 == 0:
            print(f"Epoch {epoch}/{EPOCHS}, Train Loss: {np.mean(train_losses):.4f}, Val Loss: {np.mean(val_losses):.4f}")
    return best_loss

def train_multi_step(model, train_loader, val_loader, prediction_steps=90):
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    loss_fn = torch.nn.SmoothL1Loss()
    best_loss = float('inf')
    for epoch in range(EPOCHS):
        model.train()
        train_losses = []
        for xb, yb in train_loader:
            xb = xb.to(DEVICE)
            yb = yb.to(DEVICE)
            optimizer.zero_grad()
            output = model(xb)
            # å¤„ç†ä¸åŒæ¨¡å‹çš„è¾“å‡ºæ ¼å¼
            if isinstance(output, tuple):
                preds = output[0]  # å–é¢„æµ‹å€¼
            else:
                preds = output
            if preds.dim() == 1:
                preds = preds.unsqueeze(1)
            loss = loss_fn(preds, yb[:, :preds.size(1)])
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())
        model.eval()
        val_losses = []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(DEVICE)
                yb = yb.to(DEVICE)
                output = model(xb)
                # å¤„ç†ä¸åŒæ¨¡å‹çš„è¾“å‡ºæ ¼å¼
                if isinstance(output, tuple):
                    preds = output[0]  # å–é¢„æµ‹å€¼
                else:
                    preds = output
                if preds.dim() == 1:
                    preds = preds.unsqueeze(1)
                loss = loss_fn(preds, yb[:, :preds.size(1)])
                val_losses.append(loss.item())
        if np.mean(val_losses) < best_loss:
            best_loss = np.mean(val_losses)
            torch.save(model.state_dict(), MODEL_PATH)
        if epoch % 10 == 0:
            print(f"Epoch {epoch}/{EPOCHS}, Train Loss: {np.mean(train_losses):.4f}, Val Loss: {np.mean(val_losses):.4f}")
    return best_loss

def train_recursive_model(model, train_loader, val_loader):
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    loss_fn = torch.nn.SmoothL1Loss()
    best_loss = float('inf')
    for epoch in range(EPOCHS):
        model.train()
        train_losses = []
        for xb, yb in train_loader:
            xb = xb.to(DEVICE)
            yb = yb[:, 0].to(DEVICE)  # å–ç¬¬ä¸€å¤©çš„é¢„æµ‹å€¼
            optimizer.zero_grad()
            output = model(xb)
            # å¤„ç†ä¸åŒæ¨¡å‹çš„è¾“å‡ºæ ¼å¼
            if isinstance(output, tuple):
                preds, uncertainty = output
                # å¯ä»¥é€‰æ‹©åŠ å…¥ä¸ç¡®å®šæ€§ä½œä¸ºæ­£åˆ™åŒ–é¡¹
                loss = loss_fn(preds, yb) + 0.01 * torch.mean(uncertainty)
            else:
                preds = output
                loss = loss_fn(preds, yb)
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())
        model.eval()
        val_losses = []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(DEVICE)
                yb = yb[:, 0].to(DEVICE)  # å–ç¬¬ä¸€å¤©çš„é¢„æµ‹å€¼
                output = model(xb)
                # å¤„ç†ä¸åŒæ¨¡å‹çš„è¾“å‡ºæ ¼å¼
                if isinstance(output, tuple):
                    preds, uncertainty = output
                    loss = loss_fn(preds, yb) + 0.01 * torch.mean(uncertainty)
                else:
                    preds = output
                    loss = loss_fn(preds, yb)
                val_losses.append(loss.item())
        if np.mean(val_losses) < best_loss:
            best_loss = np.mean(val_losses)
            torch.save(model.state_dict(), MODEL_PATH)
        if epoch % 10 == 0:
            print(f"Epoch {epoch}/{EPOCHS}, Train Loss: {np.mean(train_losses):.4f}, Val Loss: {np.mean(val_losses):.4f}")
    return best_loss

def train_rolling_prediction(model, train_loader, val_loader, max_steps=10):
    """ä½¿ç”¨æ”¹è¿›çš„æ»šåŠ¨é¢„æµ‹è®­ç»ƒç­–ç•¥ - é’ˆå¯¹æ®‹å·®ç½‘ç»œä¼˜åŒ–"""
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½™å¼¦é€€ç«
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE*0.1)
    loss_fn = torch.nn.SmoothL1Loss()
    best_loss = float('inf')
    patience = 30  # æ—©åœè€å¿ƒå€¼
    no_improve = 0
    
    # åŠ¨æ€è°ƒæ•´æ»šåŠ¨æ­¥æ•°å’ŒæŸå¤±æƒé‡
    step_weights = torch.tensor([1.0, 0.95, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65, 0.6, 0.55]).to(DEVICE)
    
    for epoch in range(EPOCHS):
        model.train()
        train_losses = []
        
        # åŠ¨æ€è°ƒæ•´æ»šåŠ¨æ­¥æ•°ï¼šæ›´æ¸è¿›çš„å¢é•¿
        current_max_steps = min(2 + epoch // 15, max_steps)
        
        for xb, yb in train_loader:
            xb = xb.to(DEVICE)
            yb = yb.to(DEVICE)
            optimizer.zero_grad()
            
            # æ»šåŠ¨é¢„æµ‹ï¼šé€æ­¥é¢„æµ‹å¤šä¸ªæ—¶é—´æ­¥
            rolling_loss = 0
            current_input = xb
            total_weight = 0
            
            for step in range(min(current_max_steps, yb.size(1))):
                output = model(current_input)
                if isinstance(output, tuple):
                    pred = output[0]
                    uncertainty = output[1] if len(output) > 1 else None
                else:
                    pred = output
                    uncertainty = None
                
                target = yb[:, step]
                
                # æ”¹è¿›çš„åŠ æƒæŸå¤±
                weight = step_weights[step] if step < len(step_weights) else 0.1
                step_loss = loss_fn(pred, target) * weight
                
                # ä¸ç¡®å®šæ€§æ­£åˆ™åŒ– - é¼“åŠ±æ¨¡å‹åœ¨é¢„æµ‹ä¸å‡†ç¡®æ—¶è¾“å‡ºé«˜ä¸ç¡®å®šæ€§
                if uncertainty is not None:
                    prediction_error = torch.abs(pred - target)
                    uncertainty_loss = torch.mean(torch.abs(uncertainty - prediction_error.detach()))
                    step_loss += 0.005 * uncertainty_loss * weight
                
                rolling_loss += step_loss
                total_weight += weight
                
                # æ›´æ–°è¾“å…¥ï¼šä½¿ç”¨æ›´æ™ºèƒ½çš„ç‰¹å¾æ„é€ 
                if step < current_max_steps - 1:
                    pred_expanded = pred.unsqueeze(1)
                    last_features = current_input[:, -1:, :]
                    new_features = last_features.clone()
                    new_features[:, :, 0] = pred_expanded
                    
                    # æ”¹è¿›çš„ç‰¹å¾ä¼ æ’­
                    if current_input.size(2) > 1:
                        if current_input.size(1) >= 3:  # ä½¿ç”¨æ›´é•¿çš„å†å²
                            # ä½¿ç”¨æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡è®¡ç®—è¶‹åŠ¿
                            recent_trend = 0.5 * (current_input[:, -1:, 1:] - current_input[:, -2:-1, 1:]) + \
                                         0.3 * (current_input[:, -2:-1, 1:] - current_input[:, -3:-2, 1:])
                            new_features[:, :, 1:] = current_input[:, -1:, 1:] + 0.05 * recent_trend
                        elif current_input.size(1) >= 2:
                            trend = current_input[:, -1:, 1:] - current_input[:, -2:-1, 1:]
                            new_features[:, :, 1:] = current_input[:, -1:, 1:] + 0.05 * trend
                    
                    current_input = torch.cat([current_input[:, 1:, :], new_features], dim=1)
            
            loss = rolling_loss / total_weight if total_weight > 0 else rolling_loss
            loss.backward()
            
            # è‡ªé€‚åº”æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            
            optimizer.step()
            train_losses.append(loss.item())
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_losses = []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(DEVICE)
                yb = yb.to(DEVICE)
                
                rolling_loss = 0
                current_input = xb
                total_weight = 0
                
                for step in range(min(current_max_steps, yb.size(1))):
                    output = model(current_input)
                    if isinstance(output, tuple):
                        pred = output[0]
                        uncertainty = output[1] if len(output) > 1 else None
                    else:
                        pred = output
                        uncertainty = None
                    
                    target = yb[:, step]
                    weight = step_weights[step] if step < len(step_weights) else 0.1
                    step_loss = loss_fn(pred, target) * weight
                    
                    if uncertainty is not None:
                        prediction_error = torch.abs(pred - target)
                        uncertainty_loss = torch.mean(torch.abs(uncertainty - prediction_error))
                        step_loss += 0.005 * uncertainty_loss * weight
                    
                    rolling_loss += step_loss
                    total_weight += weight
                    
                    if step < current_max_steps - 1:
                        pred_expanded = pred.unsqueeze(1)
                        last_features = current_input[:, -1:, :]
                        new_features = last_features.clone()
                        new_features[:, :, 0] = pred_expanded
                        
                        if current_input.size(2) > 1:
                            if current_input.size(1) >= 3:
                                recent_trend = 0.5 * (current_input[:, -1:, 1:] - current_input[:, -2:-1, 1:]) + \
                                             0.3 * (current_input[:, -2:-1, 1:] - current_input[:, -3:-2, 1:])
                                new_features[:, :, 1:] = current_input[:, -1:, 1:] + 0.05 * recent_trend
                            elif current_input.size(1) >= 2:
                                trend = current_input[:, -1:, 1:] - current_input[:, -2:-1, 1:]
                                new_features[:, :, 1:] = current_input[:, -1:, 1:] + 0.05 * trend
                        
                        current_input = torch.cat([current_input[:, 1:, :], new_features], dim=1)
                
                loss = rolling_loss / total_weight if total_weight > 0 else rolling_loss
                val_losses.append(loss.item())
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        # æ¨¡å‹ä¿å­˜å’Œæ—©åœ
        current_val_loss = np.mean(val_losses)
        if current_val_loss < best_loss:
            best_loss = current_val_loss
            no_improve = 0
            torch.save(model.state_dict(), MODEL_PATH)
        else:
            no_improve += 1
        
        if epoch % 10 == 0:
            current_lr = scheduler.get_last_lr()[0]
            print(f"Epoch {epoch}/{EPOCHS}, Train Loss: {np.mean(train_losses):.4f}, Val Loss: {current_val_loss:.4f}, Steps: {current_max_steps}, LR: {current_lr:.6f}")
        
        # æ—©åœ
        if no_improve >= patience:
            print(f"æ—©åœè§¦å‘äºç¬¬ {epoch} è½®ï¼ŒéªŒè¯æŸå¤±è¿ç»­ {patience} è½®æœªæ”¹å–„")
            break
    
    return best_loss

def train_teacher_forcing(model, train_loader, val_loader, teacher_forcing_ratio=0.5):
    """ä½¿ç”¨æ•™å¸ˆå¼ºåˆ¶è®­ç»ƒç­–ç•¥"""
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    loss_fn = torch.nn.SmoothL1Loss()
    best_loss = float('inf')
    
    for epoch in range(EPOCHS):
        model.train()
        train_losses = []
        for xb, yb in train_loader:
            xb = xb.to(DEVICE)
            yb = yb.to(DEVICE)
            optimizer.zero_grad()
            
            # éšæœºå†³å®šæ˜¯å¦ä½¿ç”¨æ•™å¸ˆå¼ºåˆ¶
            use_teacher_forcing = np.random.random() < teacher_forcing_ratio
            
            if use_teacher_forcing:
                # ä½¿ç”¨çœŸå®å€¼ä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥
                output = model(xb)
                if isinstance(output, tuple):
                    preds = output[0]
                else:
                    preds = output
                loss = loss_fn(preds, yb[:, 0])
            else:
                # ä½¿ç”¨é¢„æµ‹å€¼ä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥ï¼ˆç±»ä¼¼æ»šåŠ¨é¢„æµ‹ï¼‰
                output = model(xb)
                if isinstance(output, tuple):
                    preds = output[0]
                else:
                    preds = output
                loss = loss_fn(preds, yb[:, 0])
            
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())
        
        model.eval()
        val_losses = []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(DEVICE)
                yb = yb[:, 0].to(DEVICE)
                output = model(xb)
                if isinstance(output, tuple):
                    preds = output[0]
                else:
                    preds = output
                loss = loss_fn(preds, yb)
                val_losses.append(loss.item())
        
        if np.mean(val_losses) < best_loss:
            best_loss = np.mean(val_losses)
            torch.save(model.state_dict(), MODEL_PATH)
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch}/{EPOCHS}, Train Loss: {np.mean(train_losses):.4f}, Val Loss: {np.mean(val_losses):.4f}")
    
    return best_loss

# ================== è¯„ä¼°å‡½æ•° ==================
def evaluate_rolling_prediction(model, processed, processor, window_size=90, pred_days=90, verbose=False):
    """æ”¹è¿›çš„90å¤©æ»šåŠ¨é¢„æµ‹è¯„ä¼°"""
    model.eval()
    
    # è·å–åŸå§‹æµ‹è¯•æ•°æ®
    try:
        _, test_df = processor.load_data()
        test_df = processor.handle_missing(test_df)
        test_data = test_df[processor.target_column].values
        test_data_scaled = processor.target_scaler.transform(test_data.reshape(-1, 1)).flatten()
        print(f"æµ‹è¯•æ•°æ®é•¿åº¦: {len(test_data)}")
    except Exception as e:
        print(f"è·å–æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
        return np.array([]), np.array([])
    
    # è·å–æµ‹è¯•é›†çš„åŸå§‹ç‰¹å¾æ•°æ®
    try:
        # é‡æ–°å¤„ç†æµ‹è¯•æ•°æ®ä»¥è·å–å®Œæ•´çš„ç‰¹å¾
        test_features = processor.extract_features(test_df)
        test_features_scaled = processor.feature_scaler.transform(test_features)
        n_features = test_features_scaled.shape[1]
        print(f"ä½¿ç”¨å®Œæ•´ç‰¹å¾æ•°æ®ï¼Œç‰¹å¾æ•°é‡: {n_features}")
    except Exception as e:
        print(f"è·å–å®Œæ•´ç‰¹å¾å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–æ–¹æ³•: {e}")
        n_features = processed.get('n_features', 9)
        test_features_scaled = np.tile(test_data_scaled.reshape(-1, 1), (1, n_features))
    
    target_scaler = processed['target_scaler'] if 'target_scaler' in processed else processor.target_scaler
    
    # é€‰æ‹©æµ‹è¯•æ•°æ®çš„èµ·å§‹ç‚¹
    start_idx = window_size
    end_idx = len(test_data) - pred_days
    
    if end_idx <= start_idx:
        print(f"æµ‹è¯•æ•°æ®ä¸è¶³ä»¥è¿›è¡Œ{pred_days}å¤©é¢„æµ‹")
        return np.array([]), np.array([])
    
    # è·å–åˆå§‹è¾“å…¥çª—å£ï¼ˆä½¿ç”¨çœŸå®çš„å¤šç»´ç‰¹å¾ï¼‰
    initial_features = test_features_scaled[start_idx-window_size:start_idx]
    print(f"åˆå§‹ç‰¹å¾çª—å£å½¢çŠ¶: {initial_features.shape}")
    
    # è¿›è¡Œæ”¹è¿›çš„æ»šåŠ¨é¢„æµ‹
    predictions = []
    current_window = initial_features.copy()
    
    # ç”¨äºå­˜å‚¨é¢„æµ‹å†å²ï¼Œå¸®åŠ©æ”¹è¿›åç»­é¢„æµ‹
    prediction_history = []
    
    with torch.no_grad():
        for day in range(pred_days):
            # ç¡®ä¿çª—å£å¤§å°æ­£ç¡®
            if current_window.shape[0] != window_size:
                if current_window.shape[0] > window_size:
                    current_window = current_window[-window_size:]
                else:
                    padding_needed = window_size - current_window.shape[0]
                    last_row = current_window[-1:] if len(current_window) > 0 else np.zeros((1, current_window.shape[1]))
                    padding = np.repeat(last_row, padding_needed, axis=0)
                    current_window = np.vstack([current_window, padding])
            
            # å‡†å¤‡è¾“å…¥
            input_tensor = torch.FloatTensor(current_window).unsqueeze(0).to(DEVICE)
            
            # é¢„æµ‹ä¸‹ä¸€å¤©
            output = model(input_tensor)
            if isinstance(output, tuple):
                pred = output[0]
                uncertainty = output[1] if len(output) > 1 else None
            else:
                pred = output
                uncertainty = None
            
            pred_value = pred.cpu().numpy()[0]
            predictions.append(pred_value)
            prediction_history.append(pred_value)
            
            # æ„é€ ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾å‘é‡
            if day < pred_days - 1:
                # å–å½“å‰çª—å£æœ€åä¸€ä¸ªæ—¶é—´æ­¥ä½œä¸ºæ¨¡æ¿
                last_features = current_window[-1:].copy()
                
                # æ›´æ–°ç›®æ ‡å€¼ç‰¹å¾ï¼ˆå‡è®¾æ˜¯ç¬¬0ä¸ªç‰¹å¾ï¼‰
                last_features[0, 0] = pred_value
                
                # å¯¹å…¶ä»–ç‰¹å¾è¿›è¡Œæ›´æ™ºèƒ½çš„æ›´æ–°
                if current_window.shape[1] > 1:
                    # è®¡ç®—æœ€è¿‘å‡ ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾å˜åŒ–è¶‹åŠ¿
                    if len(prediction_history) >= 2:
                        # ä½¿ç”¨é¢„æµ‹å†å²æ¥ä¼°è®¡è¶‹åŠ¿
                        recent_trend = prediction_history[-1] - prediction_history[-2]
                        # å°†è¶‹åŠ¿åº”ç”¨åˆ°ç›¸å…³ç‰¹å¾ä¸Šï¼ˆæ¯”ä¾‹ç¼©æ”¾ï¼‰
                        for i in range(1, current_window.shape[1]):
                            if current_window.shape[0] >= 2:
                                feature_trend = current_window[-1, i] - current_window[-2, i]
                                # ç»“åˆå†å²è¶‹åŠ¿å’Œç‰¹å¾è¶‹åŠ¿
                                combined_trend = 0.7 * feature_trend + 0.3 * recent_trend * 0.1
                                last_features[0, i] = current_window[-1, i] + combined_trend
                    else:
                        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„é¢„æµ‹å†å²ï¼Œä½¿ç”¨ç®€å•çš„è¶‹åŠ¿å»¶ç»­
                        if current_window.shape[0] >= 2:
                            for i in range(1, current_window.shape[1]):
                                trend = current_window[-1, i] - current_window[-2, i]
                                last_features[0, i] = current_window[-1, i] + 0.5 * trend
                
                # æ›´æ–°çª—å£
                current_window = np.vstack([current_window[1:], last_features])
            
            if verbose and (day + 1) % 10 == 0:
                print(f"å·²å®Œæˆ {day + 1}/{pred_days} å¤©é¢„æµ‹")
                if uncertainty is not None:
                    print(f"  é¢„æµ‹å€¼: {pred_value:.2f}, ä¸ç¡®å®šæ€§: {uncertainty.cpu().numpy()[0]:.4f}")
    
    predictions = np.array(predictions)
    
    # è·å–çœŸå®å€¼
    true_values = test_data_scaled[start_idx:start_idx + pred_days]
    
    # åæ ‡å‡†åŒ–
    predictions_denorm = target_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()
    true_values_denorm = target_scaler.inverse_transform(true_values.reshape(-1, 1)).flatten()
    
    return predictions_denorm, true_values_denorm

def plot_comparison(y_true, y_pred, title="é¢„æµ‹ç»“æœå¯¹æ¯”", save_path=None):
    """ç»˜åˆ¶é¢„æµ‹ç»“æœå¯¹æ¯”å›¾"""
    plt.figure(figsize=(15, 6))
    plt.plot(y_true, label='çœŸå®å€¼', color='blue', alpha=0.7)
    plt.plot(y_pred, label='é¢„æµ‹å€¼', color='red', linestyle='--', alpha=0.7)
    plt.title(title, fontsize=16)
    plt.xlabel('æ—¶é—´åºåˆ—æ•°æ®ç‚¹', fontsize=12)
    plt.ylabel('æ¯æ—¥æ€»æœ‰åŠŸåŠŸç‡ (kW)', fontsize=12)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(f"{save_path}/{title}.png", dpi=300, bbox_inches='tight')
    
    plt.show()
    return mean_squared_error(y_true, y_pred), mean_absolute_error(y_true, y_pred)

# ================== ä¸»æµç¨‹ ==================
def main():
    print(f"å¼€å§‹è®­ç»ƒå®éªŒ - æ¨¡å‹ç±»å‹: {MODEL_TYPE}, è®­ç»ƒç­–ç•¥: {TRAINING_STRATEGY}")
    print(f"è®¾å¤‡: {DEVICE}")
    print(f"å°†è¿›è¡Œ {NUM_EXPERIMENTS} è½®å®éªŒï¼Œæ¯è½®è®­ç»ƒ {EPOCHS} ä¸ªepoch")
    print("=" * 80)
    
    # å­˜å‚¨æ‰€æœ‰å®éªŒç»“æœ
    all_results = {
        'rolling_mse': [],
        'rolling_mae': [],
        'experiment_details': []
    }
    
    # è·å–æ•°æ®å¤„ç†å™¨ï¼ˆåœ¨å¾ªç¯å¤–åˆ›å»ºä»¥ä¿æŒä¸€è‡´æ€§ï¼‰
    processor = TimeSeriesDataProcessor("datasets/daily_power_train2.csv", "datasets/daily_power_test2.csv", 90, 1)
    processed = processor.process_data()
    
    print(f"æ•°æ®ç‰¹å¾æ•°: {processed['n_features']}")
    # æ£€æŸ¥processedå­—å…¸ä¸­çš„å®é™…é”®
    print(f"å¯ç”¨çš„æ•°æ®é”®: {list(processed.keys())}")
    # è·å–æ•°æ®åŠ è½½å™¨æ¥æ£€æŸ¥æ•°æ®å¤§å°
    train_loader_temp, val_loader_temp = create_data_loaders(processed, batch_size=BATCH_SIZE)
    print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader_temp)}")
    print(f"éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader_temp)}")
    
    # æ£€æŸ¥æ•°æ®ç»´åº¦
    sample_x, sample_y = next(iter(train_loader_temp))
    print(f"è¾“å…¥æ•°æ®ç»´åº¦: {sample_x.shape} (batch_size, sequence_length, n_features)")
    print(f"ç›®æ ‡æ•°æ®ç»´åº¦: {sample_y.shape} (batch_size, prediction_length)")
    print(f"æ¨¡å‹å®é™…é¢„æµ‹å¤©æ•°: {sample_y.shape[1]} å¤©")
    print(f"ä½†è®­ç»ƒæ—¶åªä½¿ç”¨: ç¬¬1å¤© (ç´¢å¼•0)")
    print("-" * 80)
    
    # ç”¨äºå­˜å‚¨æœ€åä¸€è½®çš„æ»šåŠ¨é¢„æµ‹ç»“æœç”¨äºç»˜å›¾
    last_rolling_true = None
    last_rolling_pred = None
    
    # --- å¾ªç¯è¿è¡Œ5è½®å®éªŒ ---
    for experiment_num in range(NUM_EXPERIMENTS):
        print(f"\n{'='*20} ç¬¬ {experiment_num + 1} è½®å®éªŒå¼€å§‹ {'='*20}")
        experiment_start_time = np.datetime64('now')
        
        # é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨ä»¥å¢åŠ éšæœºæ€§
        train_loader, val_loader = create_data_loaders(processed, batch_size=BATCH_SIZE)
        
        # é€‰æ‹©æ¨¡å‹
        if MODEL_TYPE == "single_step":
            model = SingleStepTransformer(n_features=processed['n_features'], input_len=PRED_DAYS).to(DEVICE)
            print("âœ“ ä½¿ç”¨å•æ­¥Transformeræ¨¡å‹")
        elif MODEL_TYPE == "recursive":
            model = RecursiveTransformer(n_features=processed['n_features'], input_len=PRED_DAYS).to(DEVICE)
            print("âœ“ ä½¿ç”¨é€’å½’Transformeræ¨¡å‹")
        elif MODEL_TYPE == "multi_step":
            model = MultiStepTransformer(n_features=processed['n_features'], input_len=PRED_DAYS, output_len=90).to(DEVICE)
            print("âœ“ ä½¿ç”¨å¤šæ­¥Transformeræ¨¡å‹")
        elif MODEL_TYPE == "hybrid":
            model = HybridTransformer(n_features=processed['n_features'], input_len=PRED_DAYS).to(DEVICE)
            print("âœ“ ä½¿ç”¨æ··åˆTransformeræ¨¡å‹")
        else:
            model = SingleStepTransformer(n_features=processed['n_features'], input_len=PRED_DAYS).to(DEVICE)
            print("âœ“ ä½¿ç”¨é»˜è®¤å•æ­¥Transformeræ¨¡å‹")
        
        # è®­ç»ƒæ¨¡å‹
        print(f"\nğŸš€ å¼€å§‹ç¬¬ {experiment_num + 1} è½®è®­ç»ƒ ({EPOCHS} epochs)...")
        training_start_time = np.datetime64('now')
        
        if TRAINING_STRATEGY == "single":
            print("ğŸ“ˆ ä½¿ç”¨å•æ­¥è®­ç»ƒç­–ç•¥")
            best_loss = train_single_step(model, train_loader, val_loader)
        elif TRAINING_STRATEGY == "multi_step":
            print("ğŸ“ˆ ä½¿ç”¨å¤šæ­¥è®­ç»ƒç­–ç•¥")
            best_loss = train_multi_step(model, train_loader, val_loader)
        elif TRAINING_STRATEGY == "recursive":
            print("ğŸ“ˆ ä½¿ç”¨é€’å½’è®­ç»ƒç­–ç•¥")
            best_loss = train_recursive_model(model, train_loader, val_loader)
        elif TRAINING_STRATEGY == "rolling":
            print("ğŸ“ˆ ä½¿ç”¨æ»šåŠ¨é¢„æµ‹è®­ç»ƒç­–ç•¥")
            best_loss = train_rolling_prediction(model, train_loader, val_loader)
        elif TRAINING_STRATEGY == "teacher_forcing":
            print("ğŸ“ˆ ä½¿ç”¨æ•™å¸ˆå¼ºåˆ¶è®­ç»ƒç­–ç•¥")
            best_loss = train_teacher_forcing(model, train_loader, val_loader)
        else:
            print("ğŸ“ˆ ä½¿ç”¨é»˜è®¤å•æ­¥è®­ç»ƒç­–ç•¥")
            best_loss = train_single_step(model, train_loader, val_loader)
        
        training_end_time = np.datetime64('now')
        print(f"âœ… ç¬¬ {experiment_num + 1} è½®è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_loss:.6f}")
        
        # è¯„ä¼°90å¤©æ»šåŠ¨é¢„æµ‹æ€§èƒ½
        print(f"\nï¿½ ç¬¬ {experiment_num + 1} è½®90å¤©æ»šåŠ¨é¢„æµ‹è¯„ä¼°ä¸­...")
        eval_start_time = np.datetime64('now')
        rolling_pred, rolling_true = evaluate_rolling_prediction(
            model, processed, processor, window_size=90, pred_days=90, verbose=False
        )
        
        if len(rolling_pred) > 0 and len(rolling_true) > 0:
            rolling_mse = mean_squared_error(rolling_true, rolling_pred)
            rolling_mae = mean_absolute_error(rolling_true, rolling_pred)
            
            all_results['rolling_mse'].append(rolling_mse)
            all_results['rolling_mae'].append(rolling_mae)
            
            # å­˜å‚¨æœ€åä¸€è½®çš„æ»šåŠ¨é¢„æµ‹ç»“æœç”¨äºç»˜å›¾
            if experiment_num == NUM_EXPERIMENTS - 1:
                last_rolling_true = rolling_true
                last_rolling_pred = rolling_pred
            
            print(f"ğŸ“ˆ ç¬¬ {experiment_num + 1} è½®90å¤©æ»šåŠ¨é¢„æµ‹ç»“æœ:")
            print(f"   MSE: {rolling_mse:.6f}")
            print(f"   MAE: {rolling_mae:.6f}")
            
            rolling_success = True
        else:
            print(f"âŒ ç¬¬ {experiment_num + 1} è½®90å¤©æ»šåŠ¨é¢„æµ‹å¤±è´¥")
            rolling_success = False
            rolling_mse = rolling_mae = None
        
        eval_end_time = np.datetime64('now')
        experiment_end_time = np.datetime64('now')
        
        # è®°å½•æœ¬è½®å®éªŒè¯¦ç»†ä¿¡æ¯
        experiment_detail = {
            'experiment_num': experiment_num + 1,
            'best_validation_loss': best_loss,
            'rolling_mse': rolling_mse,
            'rolling_mae': rolling_mae,
            'rolling_success': rolling_success,
            'training_time': str(training_end_time - training_start_time),
            'evaluation_time': str(eval_end_time - eval_start_time),
            'total_time': str(experiment_end_time - experiment_start_time)
        }
        all_results['experiment_details'].append(experiment_detail)
        
        print(f"\nâ±ï¸  ç¬¬ {experiment_num + 1} è½®å®éªŒç”¨æ—¶: {experiment_detail['total_time']}")
        print(f"{'='*20} ç¬¬ {experiment_num + 1} è½®å®éªŒç»“æŸ {'='*20}")
    
    # --- è®¡ç®—å¹¶è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ç»“æœ ---
    print(f"\n{'='*80}")
    print(f"ğŸ¯ {NUM_EXPERIMENTS} è½®å®éªŒå®Œæˆ - æœ€ç»ˆç»“æœç»Ÿè®¡")
    print(f"{'='*80}")
    
    # è®¡ç®—90å¤©æ»šåŠ¨é¢„æµ‹ç»Ÿè®¡
    if all_results['rolling_mse']:
        rolling_mse_mean = np.mean(all_results['rolling_mse'])
        rolling_mse_std = np.std(all_results['rolling_mse'])
        rolling_mae_mean = np.mean(all_results['rolling_mae'])
        rolling_mae_std = np.std(all_results['rolling_mae'])
        
        print(f"\nğŸ”„ 90å¤©æ»šåŠ¨é¢„æµ‹ç»Ÿè®¡ç»“æœ:")
        print(f"   MSE - å¹³å‡å€¼: {rolling_mse_mean:.6f} Â± {rolling_mse_std:.6f}")
        print(f"   MAE - å¹³å‡å€¼: {rolling_mae_mean:.6f} Â± {rolling_mae_std:.6f}")
        print(f"   æˆåŠŸç‡: {len(all_results['rolling_mse'])}/{NUM_EXPERIMENTS} ({len(all_results['rolling_mse'])/NUM_EXPERIMENTS*100:.1f}%)")
    else:
        print(f"\nâŒ 90å¤©æ»šåŠ¨é¢„æµ‹å…¨éƒ¨å¤±è´¥")
        rolling_mse_mean = rolling_mse_std = rolling_mae_mean = rolling_mae_std = None
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
    results_filename = f"{PRED_DAYS}_{MODEL_TYPE}_{TRAINING_STRATEGY}_detailed_results.txt"
    
    with open(results_filename, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write(f"å®éªŒè¯¦ç»†ç»“æœæŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("å®éªŒé…ç½®:\n")
        f.write("-" * 40 + "\n")
        f.write(f"æ¨¡å‹ç±»å‹: {MODEL_TYPE}\n")
        f.write(f"è®­ç»ƒç­–ç•¥: {TRAINING_STRATEGY}\n")
        f.write(f"å®éªŒè½®æ•°: {NUM_EXPERIMENTS}\n")
        f.write(f"æ¯è½®è®­ç»ƒè½®æ•°: {EPOCHS}\n")
        f.write(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}\n")
        f.write(f"å­¦ä¹ ç‡: {LEARNING_RATE}\n")
        f.write(f"é¢„æµ‹å¤©æ•°: {PRED_DAYS}\n")
        f.write(f"è®¾å¤‡: {DEVICE}\n\n")
        
        f.write("æ€»ä½“ç»Ÿè®¡ç»“æœ:\n")
        f.write("-" * 40 + "\n")
        
        if rolling_mse_mean is not None:
            f.write("90å¤©æ»šåŠ¨é¢„æµ‹ç»“æœ:\n")
            f.write(f"  MSE - å¹³å‡å€¼: {rolling_mse_mean:.6f}\n")
            f.write(f"  MSE - æ ‡å‡†å·®: {rolling_mse_std:.6f}\n")
            f.write(f"  MAE - å¹³å‡å€¼: {rolling_mae_mean:.6f}\n")
            f.write(f"  MAE - æ ‡å‡†å·®: {rolling_mae_std:.6f}\n")
            f.write(f"  æˆåŠŸç‡: {len(all_results['rolling_mse'])}/{NUM_EXPERIMENTS} ({len(all_results['rolling_mse'])/NUM_EXPERIMENTS*100:.1f}%)\n\n")
        else:
            f.write("90å¤©æ»šåŠ¨é¢„æµ‹ç»“æœ: å…¨éƒ¨å®éªŒå¤±è´¥\n\n")
        
        f.write("å„è½®å®éªŒè¯¦ç»†ç»“æœ:\n")
        f.write("-" * 40 + "\n")
        for detail in all_results['experiment_details']:
            f.write(f"ç¬¬ {detail['experiment_num']} è½®å®éªŒ:\n")
            f.write(f"  æœ€ä½³éªŒè¯æŸå¤±: {detail['best_validation_loss']:.6f}\n")
            if detail['rolling_success']:
                f.write(f"  90å¤©é¢„æµ‹ MSE: {detail['rolling_mse']:.6f}\n")
                f.write(f"  90å¤©é¢„æµ‹ MAE: {detail['rolling_mae']:.6f}\n")
            else:
                f.write(f"  90å¤©é¢„æµ‹: å¤±è´¥\n")
            f.write(f"  è®­ç»ƒç”¨æ—¶: {detail['training_time']}\n")
            f.write(f"  è¯„ä¼°ç”¨æ—¶: {detail['evaluation_time']}\n")
            f.write(f"  æ€»ç”¨æ—¶: {detail['total_time']}\n")
            f.write("\n")
        
        f.write("åŸå§‹æ•°æ®:\n")
        f.write("-" * 40 + "\n")
        if all_results['rolling_mse']:
            f.write("90å¤©é¢„æµ‹ MSE: " + ", ".join([f"{x:.6f}" for x in all_results['rolling_mse']]) + "\n")
            f.write("90å¤©é¢„æµ‹ MAE: " + ", ".join([f"{x:.6f}" for x in all_results['rolling_mae']]) + "\n")
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_filename}")
    
    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
    import os
    results_dir = "results"
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)
        print(f"åˆ›å»ºç»“æœç›®å½•: {results_dir}")
    
    # ç»˜åˆ¶æœ€åä¸€è½®çš„æ»šåŠ¨é¢„æµ‹ç»“æœå¯¹æ¯”å›¾
    if last_rolling_true is not None and last_rolling_pred is not None:
        print(f"ğŸ“ˆ ç»˜åˆ¶90å¤©æ»šåŠ¨é¢„æµ‹ç»“æœå¯¹æ¯”å›¾...")
        rolling_title = f"90å¤©æ»šåŠ¨é¢„æµ‹ç»“æœå¯¹æ¯”-{MODEL_TYPE}_{TRAINING_STRATEGY}"
        plot_comparison(last_rolling_true, last_rolling_pred, 
                       title=rolling_title, save_path=results_dir)
        print(f"ğŸ’¾ 90å¤©æ»šåŠ¨é¢„æµ‹å›¾ç‰‡å·²ä¿å­˜åˆ°: {results_dir}/{rolling_title}.png")
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_PATH}")
    print("=" * 80)

if __name__ == "__main__":
    main()

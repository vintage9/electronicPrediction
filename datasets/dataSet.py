def process_to_daily(file_path, save_path=None):
    """
    ä¿®æ”¹åçš„å‡½æ•°ï¼Œå…ˆå¡«å……ç¼ºå¤±å€¼å†è¿›è¡Œæ¯æ—¥èšåˆã€‚
    Args:
        file_path: åŸå§‹åˆ†é’Ÿçº§æ•°æ®æ–‡ä»¶è·¯å¾„
        save_path: å¯é€‰ï¼Œä¿å­˜å¤„ç†åæ•°æ®çš„è·¯å¾„
    """
    df = pd.read_csv(file_path, sep=',', parse_dates=['DateTime'], infer_datetime_format=True, na_values='?', low_memory=False)

    print("å¼€å§‹å¤„ç†ç¼ºå¤±å€¼ï¼Œä¸ç®€å•èˆå¼ƒæ•°æ®...")

    # åœ¨èšåˆå‰ï¼Œå¯¹åˆ†é’Ÿçº§æ•°æ®è¿›è¡Œå¡«å……ï¼Œä»¥é¿å…å› ç¼ºå¤±å€¼è€Œä¸¢å¤±æ•´è¡Œæ•°æ®
    # é¦–å…ˆä½¿ç”¨å‰å‘å¡«å…… (ffill)
    df.fillna(method='ffill', inplace=True)
    # ç„¶åä½¿ç”¨åå‘å¡«å…… (bfill) æ¥å¤„ç†åºåˆ—å¼€å¤´çš„ç¼ºå¤±å€¼
    df.fillna(method='bfill', inplace=True)
    # æœ€åï¼Œå¦‚æœä»æœ‰ç¼ºå¤±å€¼ï¼Œç”¨åˆ—å‡å€¼å¡«å……ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
    df.fillna(df.mean(numeric_only=True), inplace=True)

    print("âœ… ç¼ºå¤±å€¼å·²å¤„ç†å®Œæ¯•ã€‚")

    df['Date'] = df['DateTime'].dt.date  # åªä¿ç•™å¹´æœˆæ—¥ä½œä¸ºæ±‡æ€»ä¾æ®

    # æŒ‰ç…§é¡¹ç›®è¦æ±‚ï¼Œè®¡ç®— sub_metering_remainder
    df['sub_metering_remainder'] = (df['Global_active_power'] * 1000 / 60) - (
            df['Sub_metering_1'] + df['Sub_metering_2'] + df['Sub_metering_3']
    )

    daily_df = df.groupby('Date').agg({
        'Global_active_power': 'sum',
        'Global_reactive_power': 'sum',
        'Voltage': 'mean',
        'Global_intensity': 'mean',
        'Sub_metering_1': 'sum',
        'Sub_metering_2': 'sum',
        'Sub_metering_3': 'sum',
        'sub_metering_remainder': 'sum',
        'RR': 'first',  # å¤©æ°”æ•°æ®å–ç¬¬ä¸€æ¡å³å¯
        'NBJRR1': 'first',
        'NBJRR5': 'first',
        'NBJRR10': 'first',
        'NBJBROU': 'first'
    }).reset_index()

    print("âœ… æ•°æ®å¤„ç†å®Œæˆï¼å…± {} å¤©è®°å½•".format(len(daily_df)))

    if save_path:
        daily_df.to_csv(save_path, index=False)
        print(f"ğŸ’¾ å·²ä¿å­˜ä¸º {save_path}")

    return daily_df


import warnings

import numpy as np
import pandas as pd
import torch
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset, DataLoader

warnings.filterwarnings('ignore')


class TimeSeriesDataProcessor:
    """æ—¶é—´åºåˆ—æ•°æ®å¤„ç†ç±»"""

    def __init__(self, train_path, test_path, sequence_length=90, prediction_length=90):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨

        Args:
            train_path: è®­ç»ƒæ•°æ®è·¯å¾„
            test_path: æµ‹è¯•æ•°æ®è·¯å¾„
            sequence_length: è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆ90å¤©ï¼‰
            prediction_length: é¢„æµ‹åºåˆ—é•¿åº¦ï¼ˆ90å¤©æˆ–365å¤©ï¼‰
        """
        self.train_path = train_path
        self.test_path = test_path
        self.sequence_length = sequence_length
        self.prediction_length = prediction_length

        # ç‰¹å¾åˆ—å
        self.feature_columns = [
            'Global_active_power', 'global_reactive_power', 'voltage',
            'global_intensity', 'sub_metering_1', 'sub_metering_2',
            'sub_metering_3', 'RR', 'NBJRR1', 'NBJRR5', 'NBJRR10', 'NBJBROU'
        ]

        # ç›®æ ‡åˆ—
        self.target_column = 'Global_active_power'

        # æ•°æ®ç¼©æ”¾å™¨
        self.feature_scaler = StandardScaler()
        self.target_scaler = StandardScaler()

        # å­˜å‚¨å¤„ç†åçš„æ•°æ®
        self.train_data = None
        self.test_data = None
        self.processed_features = None
        self.processed_targets = None

    def load_data(self):
        """åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
        print("æ­£åœ¨åŠ è½½æ•°æ®...")

        # åŠ è½½è®­ç»ƒæ•°æ®
        self.train_data = pd.read_csv(self.train_path)
        print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {self.train_data.shape}")

        # åŠ è½½æµ‹è¯•æ•°æ®
        self.test_data = pd.read_csv(self.test_path)
        print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {self.test_data.shape}")

        # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
        print("\nè®­ç»ƒæ•°æ®åŸºæœ¬ä¿¡æ¯:")
        print(self.train_data.head())
        print(f"\nç¼ºå¤±å€¼ç»Ÿè®¡:")
        print(self.train_data.isnull().sum())

        return self.train_data, self.test_data

    def handle_missing_values(self, data):
        """å¤„ç†ç¼ºå¤±å€¼"""
        print("æ­£åœ¨å¤„ç†ç¼ºå¤±å€¼...")

        # è®°å½•ç¼ºå¤±å€¼æƒ…å†µ
        missing_before = data.isnull().sum().sum()
        print(f"å¤„ç†å‰ç¼ºå¤±å€¼æ€»æ•°: {missing_before}")

        # å¯¹äºæ•°å€¼åˆ—ï¼Œä½¿ç”¨å‰å‘å¡«å…… + åå‘å¡«å……
        data_filled = data.copy()

        # é¦–å…ˆå°è¯•å‰å‘å¡«å……
        data_filled = data_filled.fillna(method='ffill')

        # ç„¶ååå‘å¡«å……ï¼ˆå¤„ç†å¼€å¤´çš„ç¼ºå¤±å€¼ï¼‰
        data_filled = data_filled.fillna(method='bfill')

        # å¦‚æœä»æœ‰ç¼ºå¤±å€¼ï¼Œä½¿ç”¨åˆ—çš„å‡å€¼å¡«å……
        for col in self.feature_columns:
            if col in data_filled.columns:
                data_filled[col] = data_filled[col].fillna(data_filled[col].mean())

        missing_after = data_filled.isnull().sum().sum()
        print(f"å¤„ç†åç¼ºå¤±å€¼æ€»æ•°: {missing_after}")

        return data_filled

    def create_derived_features(self, data):
        """åˆ›å»ºè¡ç”Ÿç‰¹å¾"""
        print("æ­£åœ¨åˆ›å»ºè¡ç”Ÿç‰¹å¾...")

        data_enhanced = data.copy()

        # è®¡ç®— sub_metering_remainder
        if all(col in data.columns for col in
               ['Global_active_power', 'sub_metering_1', 'sub_metering_2', 'sub_metering_3']):
            data_enhanced['sub_metering_remainder'] = (
                    data['Global_active_power'] * 1000 / 60 -
                    (data['sub_metering_1'] + data['sub_metering_2'] + data['sub_metering_3'])
            )
            self.feature_columns.append('sub_metering_remainder')

        # æ·»åŠ æ—¶é—´ç‰¹å¾ï¼ˆå¦‚æœæœ‰æ—¥æœŸåˆ—ï¼‰
        if 'Date' in data.columns:
            data_enhanced['Date'] = pd.to_datetime(data_enhanced['Date'])

            # æå–æ—¶é—´ç‰¹å¾
            data_enhanced['month'] = data_enhanced['Date'].dt.month
            data_enhanced['day_of_year'] = data_enhanced['Date'].dt.dayofyear
            data_enhanced['week_of_year'] = data_enhanced['Date'].dt.isocalendar().week
            data_enhanced['season'] = data_enhanced['month'].apply(self._get_season)

            # æ·»åŠ åˆ°ç‰¹å¾åˆ—è¡¨
            time_features = ['month', 'day_of_year', 'week_of_year', 'season']
            self.feature_columns.extend(time_features)

        # æ·»åŠ æ»åç‰¹å¾ï¼ˆä½¿ç”¨ç›®æ ‡å˜é‡çš„æ»åå€¼ï¼‰
        for lag in [1, 7, 30]:  # 1å¤©ã€7å¤©ã€30å¤©å‰çš„å€¼
            lag_col = f'Global_active_power_lag_{lag}'
            data_enhanced[lag_col] = data_enhanced['Global_active_power'].shift(lag)
            self.feature_columns.append(lag_col)

        # æ·»åŠ æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
        window_sizes = [7, 30]  # 7å¤©å’Œ30å¤©æ»šåŠ¨çª—å£
        for window in window_sizes:
            # æ»šåŠ¨å‡å€¼
            col_mean = f'Global_active_power_roll_mean_{window}'
            data_enhanced[col_mean] = data_enhanced['Global_active_power'].rolling(window=window).mean()
            self.feature_columns.append(col_mean)

            # æ»šåŠ¨æ ‡å‡†å·®
            col_std = f'Global_active_power_roll_std_{window}'
            data_enhanced[col_std] = data_enhanced['Global_active_power'].rolling(window=window).std()
            self.feature_columns.append(col_std)

        print(f"ç‰¹å¾æ•°é‡: {len(self.feature_columns)}")
        return data_enhanced

    def _get_season(self, month):
        """æ ¹æ®æœˆä»½è·å–å­£èŠ‚"""
        if month in [12, 1, 2]:
            return 0  # å†¬å­£
        elif month in [3, 4, 5]:
            return 1  # æ˜¥å­£
        elif month in [6, 7, 8]:
            return 2  # å¤å­£
        else:
            return 3  # ç§‹å­£

    def normalize_features(self, train_data, test_data):
        """ç‰¹å¾æ ‡å‡†åŒ–"""
        print("æ­£åœ¨è¿›è¡Œç‰¹å¾æ ‡å‡†åŒ–...")

        # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—éƒ½å­˜åœ¨
        available_features = [col for col in self.feature_columns if col in train_data.columns]
        print(f"å¯ç”¨ç‰¹å¾æ•°é‡: {len(available_features)}")

        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
        X_train = train_data[available_features].values
        y_train = train_data[self.target_column].values

        X_test = test_data[available_features].values
        y_test = test_data[self.target_column].values

        # æ ‡å‡†åŒ–ç‰¹å¾
        X_train_scaled = self.feature_scaler.fit_transform(X_train)
        X_test_scaled = self.feature_scaler.transform(X_test)

        # æ ‡å‡†åŒ–ç›®æ ‡å˜é‡
        y_train_scaled = self.target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_test_scaled = self.target_scaler.transform(y_test.reshape(-1, 1)).flatten()

        return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, available_features

    def create_sequences(self, X, y):
        """åˆ›å»ºæ—¶é—´åºåˆ—çª—å£"""
        print(f"æ­£åœ¨åˆ›å»ºæ—¶é—´åºåˆ—çª—å£ (è¾“å…¥é•¿åº¦: {self.sequence_length}, é¢„æµ‹é•¿åº¦: {self.prediction_length})...")

        sequences = []
        targets = []

        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®åˆ›å»ºåºåˆ—
        total_length = self.sequence_length + self.prediction_length
        if len(X) < total_length:
            print(f"è­¦å‘Š: æ•°æ®é•¿åº¦ {len(X)} å°äºæ‰€éœ€é•¿åº¦ {total_length}")
            return np.array([]), np.array([])

        # åˆ›å»ºæ»‘åŠ¨çª—å£
        for i in range(len(X) - total_length + 1):
            # è¾“å…¥åºåˆ—
            seq = X[i:i + self.sequence_length]
            # ç›®æ ‡åºåˆ—
            target = y[i + self.sequence_length:i + self.sequence_length + self.prediction_length]

            sequences.append(seq)
            targets.append(target)

        print(f"åˆ›å»ºäº† {len(sequences)} ä¸ªåºåˆ—")
        return np.array(sequences), np.array(targets)

    def process_data(self):
        """å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹"""
        print("å¼€å§‹æ•°æ®å¤„ç†æµç¨‹...")

        # 1. åŠ è½½æ•°æ®
        train_data, test_data = self.load_data()

        # 2. å¤„ç†ç¼ºå¤±å€¼
        train_data = self.handle_missing_values(train_data)
        test_data = self.handle_missing_values(test_data)

        # 3. åˆ›å»ºè¡ç”Ÿç‰¹å¾
        train_data = self.create_derived_features(train_data)
        test_data = self.create_derived_features(test_data)

        # 4. å†æ¬¡å¤„ç†ç¼ºå¤±å€¼ï¼ˆè¡ç”Ÿç‰¹å¾å¯èƒ½äº§ç”Ÿæ–°çš„ç¼ºå¤±å€¼ï¼‰
        train_data = self.handle_missing_values(train_data)
        test_data = self.handle_missing_values(test_data)

        # 5. ç‰¹å¾æ ‡å‡†åŒ–
        X_train, X_test, y_train, y_test, feature_names = self.normalize_features(train_data, test_data)

        # 6. åˆ›å»ºæ—¶é—´åºåˆ—çª—å£
        X_train_seq, y_train_seq = self.create_sequences(X_train, y_train)
        X_test_seq, y_test_seq = self.create_sequences(X_test, y_test)

        # ä¿å­˜å¤„ç†ç»“æœ
        self.processed_data = {
            'X_train': X_train_seq,
            'y_train': y_train_seq,
            'X_test': X_test_seq,
            'y_test': y_test_seq,
            'feature_names': feature_names,
            'n_features': len(feature_names)
        }

        print("æ•°æ®å¤„ç†å®Œæˆ!")
        print(f"è®­ç»ƒé›†å½¢çŠ¶: X={X_train_seq.shape}, y={y_train_seq.shape}")
        print(f"æµ‹è¯•é›†å½¢çŠ¶: X={X_test_seq.shape}, y={y_test_seq.shape}")

        return self.processed_data

    def inverse_transform_target(self, scaled_predictions):
        """åæ ‡å‡†åŒ–ç›®æ ‡å˜é‡"""
        return self.target_scaler.inverse_transform(scaled_predictions.reshape(-1, 1)).flatten()

    def get_data_info(self):
        """è·å–æ•°æ®ä¿¡æ¯"""
        if self.processed_data is None:
            print("è¯·å…ˆè¿è¡Œ process_data() æ–¹æ³•")
            return None

        info = {
            'sequence_length': self.sequence_length,
            'prediction_length': self.prediction_length,
            'n_features': self.processed_data['n_features'],
            'feature_names': self.processed_data['feature_names'],
            'train_samples': len(self.processed_data['X_train']),
            'test_samples': len(self.processed_data['X_test'])
        }

        return info


class TimeSeriesDataset(Dataset):
    """PyTorchæ•°æ®é›†ç±»"""

    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


def create_data_loaders(processed_data, batch_size=32, shuffle=True):
    """åˆ›å»ºPyTorchæ•°æ®åŠ è½½å™¨"""

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = TimeSeriesDataset(processed_data['X_train'], processed_data['y_train'])
    test_dataset = TimeSeriesDataset(processed_data['X_test'], processed_data['y_test'])

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader


# # ä½¿ç”¨ç¤ºä¾‹
# if __name__ == "__main__":
#     # çŸ­æœŸé¢„æµ‹ï¼ˆ90å¤©ï¼‰
#     processor_short = TimeSeriesDataProcessor(
#         train_path='train.csv',
#         test_path='test.csv',
#         sequence_length=90,
#         prediction_length=90
#     )
#
#     # å¤„ç†æ•°æ®
#     processed_data_short = processor_short.process_data()
#
#     # åˆ›å»ºæ•°æ®åŠ è½½å™¨
#     train_loader_short, test_loader_short = create_data_loaders(processed_data_short, batch_size=32)
#
#     # æ‰“å°æ•°æ®ä¿¡æ¯
#     data_info = processor_short.get_data_info()
#     print("\næ•°æ®ä¿¡æ¯:")
#     for key, value in data_info.items():
#         print(f"{key}: {value}")
#
#     # é•¿æœŸé¢„æµ‹ï¼ˆ365å¤©ï¼‰
#     processor_long = TimeSeriesDataProcessor(
#         train_path='train.csv',
#         test_path='test.csv',
#         sequence_length=90,
#         prediction_length=365
#     )
#
#     # å¤„ç†æ•°æ®
#     processed_data_long = processor_long.process_data()
#
#     # åˆ›å»ºæ•°æ®åŠ è½½å™¨
#     train_loader_long, test_loader_long = create_data_loaders(processed_data_long, batch_size=16)  # é•¿æœŸé¢„æµ‹ä½¿ç”¨è¾ƒå°batch_size


if __name__ == "__main__":
    daily_data = process_to_daily("train.csv", save_path="daily_power_train2.csv")
    daily_data2 = process_to_daily("test_with_header.csv", save_path="daily_power_test2.csv")


